{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRser type is: \n",
      " <class 'pandas.core.series.Series'>\n",
      "BRser is:\n",
      " 1_Country                 Brazil\n",
      "2_Capital               Brasilia\n",
      "3_Currency        Brazilian real\n",
      "4_Currency_Sym                 R\n",
      "Name: BR, dtype: object\n",
      "\n",
      "Curr is: \n",
      " BR         Brazilian real\n",
      "RU          Russian Ruble\n",
      "IN           Indian Rupee\n",
      "CH           Chinese Yuan\n",
      "SA    South African Rand \n",
      "Name: 3_Currency, dtype: object\n",
      "\n",
      "   1_Country  2_Capital   3_Currency 4_Currency_Sym\n",
      "BR    Brazil   Brasilia    R, Brazil              R\n",
      "RU    Russia     Moscow    ₽, Russia              ₽\n",
      "IN     India  New Dehli     ₹, India              ₹\n",
      "CH     China    Beijing     ¥, China              ¥\n",
      "SA  SouthAfr   Pretoria  R, SouthAfr              R\n",
      "extracts element in row 0 under column 2_Capital using iloc() and at \n",
      "\n",
      "Brasilia\n",
      "extracts element in row 0 under column 2_Capital using iat\n",
      "Brasilia\n"
     ]
    }
   ],
   "source": [
    "# Practice Problem 2.1a - extracting a column & print\n",
    "import pandas as pd\n",
    "mydict = {\"1_Country\": [\"Brazil\", \"Russia\", \"India\", \"China\", \"SouthAfr\"],\n",
    "       \"2_Capital\": [\"Brasilia\", \"Moscow\", \"New Dehli\", \"Beijing\", \"Pretoria\"],\n",
    "       \"3_Currency\": [\"Brazilian real\", \"Russian Ruble\", \"Indian Rupee\", \"Chinese Yuan\", \"South African Rand \"],\n",
    "       \"4_Currency_Sym\": [\"R\", \"₽\", \"₹\", \"¥\", \"R\"] }\n",
    "country_info = pd.DataFrame(mydict) # convert into a dataframe\n",
    "\n",
    "# print('country_info[1:] will print:\\n ',country_info[1:])  # see what this prints\n",
    "country_info.index = [\"BR\", \"RU\", \"IN\", \"CH\", \"SA\"] # replacing the left most column with custom indexes\n",
    "\n",
    "country_info.loc['BR']  # this should extract all cols in row 'BR' - Check.\n",
    "\n",
    "BRser = country_info.loc['BR'] # loc() method \n",
    "print('BRser type is: \\n',type(BRser))\n",
    "print('BRser is:\\n',BRser) # Chk what it prints  \n",
    "print()\n",
    "\n",
    "Curr = country_info[\"3_Currency\"]\n",
    "print('Curr is: \\n', Curr)\n",
    "print()\n",
    "# Practice Problem 2.1b - extract and concat\n",
    "\n",
    "new = country_info[\"1_Country\"].copy() # making a copy of Country Feature column\n",
    "country_info[\"3_Currency\"] = country_info[\"4_Currency_Sym\"].str.cat(new, sep=\", \") # concat Currency_Sym with Currency\n",
    "\n",
    "print(country_info)\n",
    "\n",
    "print('extracts element in row 0 under column 2_Capital using iloc() and at \\n')\n",
    "print(country_info.iloc[0].at[\"2_Capital\"])\n",
    "\n",
    "print('extracts element in row 0 under column 2_Capital using iat')\n",
    "print(country_info.iat[0,1])\n",
    "# extracts element in row 0 under column 2_Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "       chest      weight\n",
      "age                     \n",
      "22    94.700  163.625000\n",
      "23    95.350  175.100000\n",
      "24    99.125  189.750000\n",
      "25   100.925  179.312500\n",
      "26   103.950  194.833333\n",
      "37.3\n"
     ]
    }
   ],
   "source": [
    "# Practice Problem 2.2 - Reading a csv file and related commands\n",
    "import pandas as pd\n",
    "BFA = pd.read_csv('bodyfat.csv')\n",
    "# print(BFA[2:3]) # accessing the 2nd row - same as slicing mechanism\n",
    "# print(BFA.at[3, \"knee\"]) \n",
    "\n",
    "# X = BFA.groupby(['density'])[['bodyfat']].describe()\n",
    "# print(X)\n",
    "\n",
    "# chestFatGroup = BFA.groupby(\"age\")[\"chest\"].mean()\n",
    "# print(chestFatGroup)\n",
    "\n",
    "multiGroup = BFA.groupby('age')[['chest', 'weight']].mean()\n",
    "print(type(multiGroup))\n",
    "print(multiGroup[0:5])\n",
    "\n",
    "print(BFA.iloc[3].at[\"knee\"]) # - try printing this and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by Category for bodyfat stats: \n",
      "     bodyfat                                                         \n",
      "      count       mean        std   min     25%    50%     75%   max\n",
      "age                                                                 \n",
      "22      2.0  15.700000  13.576450   6.1  10.900  15.70  20.500  25.3\n",
      "23      4.0  10.925000   1.317510   9.4  10.075  11.00  11.850  12.3\n",
      "24      4.0  20.750000   6.014704  14.2  17.950  20.05  22.850  28.7\n",
      "25      4.0  12.850000  11.752021   4.1   5.000   8.85  16.700  29.6\n",
      "26      6.0  15.150000   7.178231   7.1   9.650  14.80  19.500  25.2\n",
      "27      7.0  10.657143   6.425174   3.7   5.750  10.10  14.150  21.0\n",
      "28      7.0  17.542857   6.662296  10.0  14.600  16.00  18.200  31.2\n",
      "29      2.0   7.250000   2.192031   5.7   6.475   7.25   8.025   8.8\n",
      "30      2.0  16.850000   6.151829  12.5  14.675  16.85  19.025  21.2\n",
      "31      4.0  17.600000   6.411968   9.4  14.050  19.05  22.600  22.9\n",
      "32      4.0  18.325000   4.786352  11.9  16.250  19.25  21.325  22.9\n",
      "33      3.0  14.700000   1.752142  13.0  13.800  14.60  15.550  16.5\n",
      "34      4.0  19.825000  10.076168   7.9  13.300  21.20  27.725  29.0\n",
      "35     10.0  17.930000   9.688602   0.7  17.175  20.40  21.575  34.3\n",
      "36      2.0  21.100000   5.939697  16.9  19.000  21.10  23.200  25.3\n",
      "37      3.0  17.633333  10.743060   9.9  11.500  13.10  21.500  29.9\n",
      "38      2.0  16.050000   9.121677   9.6  12.825  16.05  19.275  22.5\n",
      "39      5.0  15.720000   8.992608   5.6   7.700  16.90  21.800  26.6\n",
      "40     17.0  16.411765   7.284391   0.0  11.500  17.50  22.400  25.8\n",
      "41     10.0  23.040000   6.061023  11.4  20.425  22.45  24.400  32.3\n",
      "42     12.0  20.050000   9.205581   3.9  15.650  21.65  24.900  38.1\n",
      "43     13.0  21.153846   5.582355  12.2  17.400  19.70  25.400  29.4\n",
      "44      9.0  23.544444   9.563617   6.0  18.100  25.30  32.900  34.8\n",
      "45      2.0  24.050000  14.778532  13.6  18.825  24.05  29.275  34.5\n",
      "46      6.0  20.700000   8.948072   8.3  17.575  19.15  23.800  35.2\n",
      "47     11.0  15.345455   8.728272   4.0   9.300  10.80  22.150  32.8\n",
      "48      5.0  22.580000   6.729562  14.1  20.100  20.40  26.700  31.6\n",
      "49      9.0  20.244444  10.125230   6.3  18.100  20.40  22.300  40.1\n",
      "50      7.0  21.485714   7.822495  11.3  16.250  19.50  27.250  32.6\n",
      "51      5.0  18.100000  16.707932   7.5   8.000  13.60  13.900  47.5\n",
      "52      4.0  23.500000   3.976598  18.3  22.050  23.90  25.350  27.9\n",
      "53      2.0  16.200000   1.697056  15.0  15.600  16.20  16.800  17.4\n",
      "54      8.0  20.312500   9.324842   6.3  12.175  22.05  27.175  31.5\n",
      "55     10.0  14.760000   7.409783   5.2  11.300  13.20  14.550  30.0\n",
      "56      4.0  17.550000   8.931032   8.5  13.300  15.95  20.200  29.8\n",
      "57      4.0  16.950000  10.694079   8.8  10.150  13.35  20.150  32.3\n",
      "58      3.0  20.833333   5.662449  15.4  17.900  20.40  23.550  26.7\n",
      "60      1.0  25.800000        NaN  25.8  25.800  25.80  25.800  25.8\n",
      "61      4.0  20.175000   6.433441  11.8  16.825  21.55  24.900  25.8\n",
      "62      5.0  24.360000   3.523209  18.6  24.300  24.80  26.100  28.0\n",
      "63      1.0  27.300000        NaN  27.3  27.300  27.30  27.300  27.3\n",
      "64      4.0  20.175000   6.219526  12.4  16.900  20.75  24.025  26.8\n",
      "65      3.0  27.300000   9.277392  17.0  23.450  29.90  32.450  35.0\n",
      "66      2.0  24.600000   8.202439  18.8  21.700  24.60  27.500  30.4\n",
      "67      4.0  29.900000   2.660827  26.6  28.400  30.20  31.700  32.6\n",
      "68      1.0  15.200000        NaN  15.2  15.200  15.20  15.200  15.2\n",
      "69      2.0  26.200000   5.656854  22.2  24.200  26.20  28.200  30.2\n",
      "70      2.0  19.000000  11.313708  11.0  15.000  19.00  23.000  27.0\n",
      "72      5.0  26.160000   6.941398  14.9  26.000  27.00  29.300  33.6\n",
      "74      1.0  31.900000        NaN  31.9  31.900  31.90  31.900  31.9\n",
      "81      1.0  21.500000        NaN  21.5  21.500  21.50  21.500  21.5\n",
      "For categories 4 and 5:     bodyfat                                                     \n",
      "      count    mean        std   min     25%    50%    75%   max\n",
      "age                                                             \n",
      "22      2.0  15.700  13.576450   6.1  10.900  15.70  20.50  25.3\n",
      "23      4.0  10.925   1.317510   9.4  10.075  11.00  11.85  12.3\n",
      "24      4.0  20.750   6.014704  14.2  17.950  20.05  22.85  28.7\n",
      "25      4.0  12.850  11.752021   4.1   5.000   8.85  16.70  29.6\n",
      "26      6.0  15.150   7.178231   7.1   9.650  14.80  19.50  25.2\n",
      "<class 'pandas.core.series.Series'>\n",
      "Grouping accorning to age using average fat in the chest: \n",
      " age\n",
      "22     94.700\n",
      "23     95.350\n",
      "24     99.125\n",
      "25    100.925\n",
      "26    103.950\n",
      "Name: chest, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Multiple Grouping : \n",
      "        chest      weight\n",
      "age                     \n",
      "22    94.700  163.625000\n",
      "23    95.350  175.100000\n",
      "24    99.125  189.750000\n",
      "25   100.925  179.312500\n",
      "26   103.950  194.833333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_l/9yt9fjns3yl8khq3hlylkj7w0000gn/T/ipykernel_12681/102593858.py:18: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  multiGroup = BFA.groupby(\"age\")[\"chest\",\"weight\"].mean()\n"
     ]
    }
   ],
   "source": [
    "# Practice Problem 2.2 cont\n",
    "import pandas as pd\n",
    "BFA = pd.read_csv('bodyfat.csv')\n",
    "X = BFA.groupby(['age'])[['bodyfat']].describe()\n",
    "print('Grouping by Category for bodyfat stats: \\n', X)   \n",
    "\n",
    "print('For categories 4 and 5:', X[0:5])\n",
    "\n",
    "chestFatgroup = BFA.groupby(\"age\")[\"chest\"].mean()\n",
    "print(type(chestFatgroup))\n",
    "print('Grouping accorning to age using average fat in the chest: \\n', chestFatgroup[:5])\n",
    "\n",
    "# Multiple aggregation of data as groups\n",
    "# If we want to group more than one quantity, we can do by listing\n",
    "# them in the value col parameter w.r.t a specific label col parameter\n",
    "# W.r.t \"age\" we want to group, chest and weight info.\n",
    "\n",
    "multiGroup = BFA.groupby(\"age\")[\"chest\",\"weight\"].mean()\n",
    "print(type(multiGroup))\n",
    "print('Multiple Grouping : \\n', multiGroup[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "<class 'pandas.core.indexes.base.Index'>\n",
      "Index(['First Name', 'Gender', 'Start Date', 'Last Login Time', 'Salary',\n",
      "       'Bonus %', 'Senior Management', 'Team'],\n",
      "      dtype='object')\n",
      "['FIRST NAME', 'GENDER', 'START DATE', 'LAST LOGIN TIME', 'SALARY', 'BONUS %', 'SENIOR MANAGEMENT', 'TEAM']\n"
     ]
    }
   ],
   "source": [
    "# problem 2.3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "smallData = pd.read_csv('EmpSmalldata0.csv')\n",
    "print(smallData.shape)\n",
    "\n",
    "colNames = smallData.columns # colNames is a LIST - features of the dataset\n",
    "print(type(colNames))\n",
    "print(colNames)\n",
    "\n",
    "colNames = [col.upper() for col in smallData] # list comprehension to convert colNames to uppercase\n",
    "print(colNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracts element in row 0 under column 2_Capital using iloc() and at \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'country_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 55\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m# print('Top 5 rows: \\n',empDF.head(5))\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# print('Features of this dataset: \\n', empDF.columns)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# print('Shape of the employees DF: \\n',empDF.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# #print('Z2 is a: \\n',type(Z2))\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# print()\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mextracts element in row 0 under column 2_Capital using iloc() and at \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[39mprint\u001b[39m(country_info\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mat[\u001b[39m\"\u001b[39m\u001b[39m2_Capital\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'country_info' is not defined"
     ]
    }
   ],
   "source": [
    "# problem 2.4\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# empDF = pd.read_csv('employees.csv')\n",
    "# # print(empDF)\n",
    "\n",
    "# # # PART 1 - Use of Index and value_count() methods\n",
    "# mylist = empDF[\"First Name\"] # this will give a series \n",
    "# print(mylist) # for debugging\n",
    "# print(type(mylist))\n",
    "# print()\n",
    "\n",
    "# L = empDF[\"First Name\"].tolist()  # series to list\n",
    "# print(\"Printing from the list L\", L[0:5]) # for debugging\n",
    "# print()\n",
    "\n",
    "# idx = pd.Index(L)\n",
    "# print(idx)\n",
    "\n",
    "# # Now using this idx, we can use value_counts()-counts the\n",
    "# # freq of items in descending order\n",
    "\n",
    "# X = idx.value_counts() \n",
    "# print('Duplicates: \\n',X.head())\n",
    "\n",
    "\n",
    "# PART 2 - Use of duplicated() method\n",
    "# empDF1 = pd.read_csv('EmpSmalldata1.csv')\n",
    "# duplicatedDF = empDF1[empDF1.duplicated('First Name')]\n",
    "# print(duplicatedDF)\n",
    "\n",
    "# duplicateRowsColsDF = empDF1[empDF1.duplicated(['First Name', 'Salary'])]\n",
    "# print(\"Duplicate Rows based on specific cols except first occurrence based on specific columns are :\")\n",
    "# print(duplicateRowsColsDF)\n",
    "\n",
    "empDF = pd.read_csv(\"employees.csv\", index_col = 'First Name')\n",
    "# print('Top 5 rows: \\n',empDF.head(5))\n",
    "# print('Features of this dataset: \\n', empDF.columns)\n",
    "# print('Shape of the employees DF: \\n',empDF.shape)\n",
    "# print()\n",
    "\n",
    "# # print('Using loc() function:\\n', empDF.loc['Douglas'])\n",
    "# print()\n",
    "# print('Using iloc() function to extract Row 3: \\n', empDF.iloc[3])\n",
    "\n",
    "# Z1 = empDF[10:16]  # from the original empDF\n",
    "# #print('Z1 is: \\n', Z1) # you need not print this; just for debug\n",
    "# Z2 = Z1[['Salary', 'Team', 'Senior Management']] \n",
    "# print('Z2 (rows 10 to 15) is: \\n',Z2)\n",
    "# #print('Z2 is a: \\n',type(Z2))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESD -  Shape of the data: \n",
      " (19, 8)\n",
      "\n",
      "Index method gives:  RangeIndex(start=0, stop=19, step=1)\n",
      "\n",
      "Features of this ESD: \n",
      " Index(['First Name', 'Gender', 'Start Date', 'Last Login Time', 'Salary',\n",
      "       'Bonus %', 'Senior Management', 'Team'],\n",
      "      dtype='object')\n",
      "\n",
      "Female Gender Stats: \n",
      "    First Name  Gender  Start Date Last Login Time    Salary  Bonus %  \\\n",
      "2       Maria  Female   4/23/1993        11:17 am  130590.0   11.858   \n",
      "6        Ruby  Female   8/17/1987         4:20 pm   65476.0   10.012   \n",
      "7         NaN  Female   7/20/2015        10:43 am   45906.0      NaN   \n",
      "10     Louise  Female   8/12/1980         9:01 am       NaN   15.132   \n",
      "11      Julie  Female  10/26/1997         3:19 pm  102508.0   12.637   \n",
      "14   Kimberly  Female   1/14/1999         7:13 am   41426.0   14.543   \n",
      "15    Lillian  Female    6/5/2016         6:09 am       NaN    1.256   \n",
      "\n",
      "   Senior Management     Team  \n",
      "2              False  Finance  \n",
      "6               True  Product  \n",
      "7                NaN  Finance  \n",
      "10              True      NaN  \n",
      "11              True    Legal  \n",
      "14              True  Finance  \n",
      "15             False  Product  \n",
      "\n",
      "Female Gender Stats: \n",
      "    First Name  Start Date Last Login Time    Salary  Bonus %  \\\n",
      "2       Maria   4/23/1993        11:17 am  130590.0   11.858   \n",
      "6        Ruby   8/17/1987         4:20 pm   65476.0   10.012   \n",
      "7         NaN   7/20/2015        10:43 am   45906.0      NaN   \n",
      "10     Louise   8/12/1980         9:01 am       NaN   15.132   \n",
      "11      Julie  10/26/1997         3:19 pm  102508.0   12.637   \n",
      "14   Kimberly   1/14/1999         7:13 am   41426.0   14.543   \n",
      "15    Lillian    6/5/2016         6:09 am       NaN    1.256   \n",
      "\n",
      "   Senior Management     Team  \n",
      "2              False  Finance  \n",
      "6               True  Product  \n",
      "7                NaN  Finance  \n",
      "10              True      NaN  \n",
      "11              True    Legal  \n",
      "14              True  Finance  \n",
      "15             False  Product  \n",
      "\n",
      "ESDnull is: \n",
      "    First Name  Gender  Start Date  Last Login Time  Salary  Bonus %  \\\n",
      "0       False   False       False            False   False    False   \n",
      "1       False   False       False            False   False    False   \n",
      "2       False   False       False            False   False    False   \n",
      "3       False    True       False            False   False    False   \n",
      "4       False   False       False            False   False     True   \n",
      "5       False   False       False            False    True    False   \n",
      "6       False   False       False            False   False    False   \n",
      "7        True   False       False            False   False     True   \n",
      "8       False    True       False            False   False    False   \n",
      "9       False    True       False            False   False    False   \n",
      "\n",
      "   Senior Management   Team  \n",
      "0              False  False  \n",
      "1              False   True  \n",
      "2              False  False  \n",
      "3              False  False  \n",
      "4              False  False  \n",
      "5              False  False  \n",
      "6              False  False  \n",
      "7               True  False  \n",
      "8              False  False  \n",
      "9              False  False  \n",
      "\n",
      "Number of missing data in each feature: \n",
      "\n",
      "<class 'pandas.core.series.Series'>\n",
      "First Name           1\n",
      "Gender               5\n",
      "Start Date           0\n",
      "Last Login Time      0\n",
      "Salary               5\n",
      "Bonus %              4\n",
      "Senior Management    2\n",
      "Team                 2\n",
      "dtype: int64\n",
      "\n",
      "Columns to delete in ESD are:  ['Gender', 'Salary', 'Bonus %']\n",
      "\n",
      "After removing identified columns - EmpSmalldata0  Shape: \n",
      " (19, 5)\n",
      "  First Name Start Date Last Login Time Senior Management             Team\n",
      "0    Douglas   8/6/1993        12:42 pm              True        Marketing\n",
      "1     Thomas  3/31/1996         6:53 am              True              NaN\n",
      "2      Maria  4/23/1993        11:17 am             False          Finance\n",
      "3      Jerry   3/4/2005         1:00 pm              True          Finance\n",
      "4      Larry  1/24/1998         4:47 pm              True  Client Services\n"
     ]
    }
   ],
   "source": [
    "# Problem 2.5\n",
    "# Handling missing data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ESD = pd.read_csv(\"EmpSmalldata.csv\")\n",
    "print('ESD -  Shape of the data: \\n',ESD.shape)\n",
    "print()\n",
    "print('Index method gives: ', ESD.index)\n",
    "print()\n",
    "\n",
    "ESDcols = ESD.columns\n",
    "print(\"Features of this ESD: \\n\",ESDcols)\n",
    "print()\n",
    "\n",
    "# testing - extract female statistics alone\n",
    "print('Female Gender Stats: \\n', ESD[ESD.Gender == 'Female'])\n",
    "print()\n",
    "# Sometime we need to remove and present the data by removing\n",
    "# the Gender col from this ESD and print as all are females;\n",
    "Xf = ESD[ESD.Gender == 'Female']\n",
    "print('Female Gender Stats: \\n', Xf.drop(['Gender'], axis=1))\n",
    "print()\n",
    "\n",
    "#Identifying the gaps/voids in the data\n",
    "\n",
    "ESDnull = ESD.isnull() # isnull() method identifies the voids, if any\n",
    "# ESDnull is also a DF\n",
    "\n",
    "print(\"ESDnull is: \\n\", ESDnull.head(10))  # What do you observe?\n",
    "print()\n",
    "\n",
    "print(\"Number of missing data in each feature: \\n\")\n",
    "missingdata_stats = ESD.isnull().sum()\n",
    "print(type(missingdata_stats)) # what do you observe?\n",
    "print(missingdata_stats)\n",
    "print()\n",
    "\n",
    "# Now, use the above missingdata_stats and remove those columns\n",
    "# in which there are more than 3 missing values - applic req\n",
    "\n",
    "# print(missingdata_stats[\"Salary\"]) # just checking!\n",
    "\n",
    "f = [] # creating a feature list to delete from ESD\n",
    "for i in range(len(missingdata_stats)):\n",
    "       if(missingdata_stats[i] >= 3):\n",
    "              # print(missingdata_stats[i]) # this will print the value\n",
    "              feature = ESDcols[i] # extracting the name of the feature here\n",
    "              # print(\"Column to delete in ESD: \",feature)\n",
    "              f.append(feature)\n",
    "print(\"Columns to delete in ESD are: \",f)\n",
    "\n",
    "# above is a detailed implementation; You can try writing with comprehensions\n",
    "\n",
    "print()\n",
    "for j in range(len(f)):\n",
    "       ESD = ESD.drop(f[j],axis=1)\n",
    "\n",
    "print('After removing identified columns - EmpSmalldata0  Shape: \\n',ESD.shape)\n",
    "print(ESD.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X       Y       Q     M   Z\n",
      "0  potato  carrot  potato  nuts   7\n",
      "1  carrot  butter  potato   NaN   9\n",
      "2    nuts    nuts  potato  nuts  13\n",
      "    Z  col1_carrot  col1_nuts  col1_potato  col2_butter  col2_carrot  \\\n",
      "0   7            0          0            1            0            1   \n",
      "1   9            1          0            0            1            0   \n",
      "2  13            0          1            0            0            0   \n",
      "\n",
      "   col2_nuts  col3_potato  col4_nuts  \n",
      "0          0            1          1  \n",
      "1          0            1          0  \n",
      "2          1            1          1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = pd.DataFrame({'X': ['potato', 'carrot', 'nuts'], 'Y': ['carrot', 'butter', 'nuts'],\n",
    "                   'Q': ['potato','potato','potato'], 'M': ['nuts',np.NaN,'nuts'], 'Z': [7,9,13]})\n",
    "print(df)\n",
    "\n",
    "X = pd.get_dummies(df, prefix=['col1', 'col2', 'col3','col4'])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      EmployeeName    Department  HireDate Sex   Birthdate  Weight  Height  \\\n",
      "0     Xavier Simon    Accounting      2010   M  04/09/1982      78     176   \n",
      "1      Sarah Nayer   Engineering      2018   F  14/04/1981      80     160   \n",
      "2     Andrea Salim   Engineering      2012   F  06/05/1997      66     169   \n",
      "3      Kasim Akram            HR      2014   M  08/01/1986      67     157   \n",
      "4  Henry Sebastein            HR      2014   M  10/10/1988      90     185   \n",
      "5    Amenda Bailey            HR      2018   F  12/11/1992      57     164   \n",
      "6       Susan Paul  Data Science      2020   F  10/04/1991     115     195   \n",
      "7       Tham Khong  Data Science      2018   M  16/07/1995      87     180   \n",
      "8       Alex Chang    Accounting      2020   M  08/10/1992      95     174   \n",
      "9         Kim Wang  Data Science      2012   F  11/10/1979      57     165   \n",
      "\n",
      "   Kids  \n",
      "0     2  \n",
      "1     1  \n",
      "2     0  \n",
      "3     1  \n",
      "4     1  \n",
      "5     0  \n",
      "6     2  \n",
      "7     0  \n",
      "8     3  \n",
      "9     1  \n",
      "\n",
      "      EmployeeName    Department  HireDate Sex   Birthdate  Weight  Height  \\\n",
      "0     Xavier Simon    Accounting      2010   M  04/09/1982      78     176   \n",
      "1      Sarah Nayer   Engineering      2018   F  14/04/1981      80     160   \n",
      "2     Andrea Salim   Engineering      2012   F  06/05/1997      66     169   \n",
      "3      Kasim Akram            HR      2014   M  08/01/1986      67     157   \n",
      "4  Henry Sebastein            HR      2014   M  10/10/1988      90     185   \n",
      "5    Amenda Bailey            HR      2018   F  12/11/1992      57     164   \n",
      "6       Susan Paul  Data Science      2020   F  10/04/1991     115     195   \n",
      "7       Tham Khong  Data Science      2018   M  16/07/1995      87     180   \n",
      "8       Alex Chang    Accounting      2020   M  08/10/1992      95     174   \n",
      "9         Kim Wang  Data Science      2012   F  11/10/1979      57     165   \n",
      "\n",
      "   Kids FirstName   LastName  \n",
      "0     2    Xavier      Simon  \n",
      "1     1     Sarah      Nayer  \n",
      "2     0    Andrea      Salim  \n",
      "3     1     Kasim      Akram  \n",
      "4     1     Henry  Sebastein  \n",
      "5     0    Amenda     Bailey  \n",
      "6     2     Susan       Paul  \n",
      "7     0      Tham      Khong  \n",
      "8     3      Alex      Chang  \n",
      "9     1       Kim       Wang  \n",
      "\n",
      "      EmployeeName    Department  HireDate Sex   Birthdate  Weight  Height  \\\n",
      "0     Xavier Simon    Accounting      2010   M  04/09/1982      78     176   \n",
      "1      Sarah Nayer   Engineering      2018   F  14/04/1981      80     160   \n",
      "2     Andrea Salim   Engineering      2012   F  06/05/1997      66     169   \n",
      "3      Kasim Akram            HR      2014   M  08/01/1986      67     157   \n",
      "4  Henry Sebastein            HR      2014   M  10/10/1988      90     185   \n",
      "5    Amenda Bailey            HR      2018   F  12/11/1992      57     164   \n",
      "6       Susan Paul  Data Science      2020   F  10/04/1991     115     195   \n",
      "7       Tham Khong  Data Science      2018   M  16/07/1995      87     180   \n",
      "8       Alex Chang    Accounting      2020   M  08/10/1992      95     174   \n",
      "9         Kim Wang  Data Science      2012   F  11/10/1979      57     165   \n",
      "\n",
      "   Kids FirstName   LastName  Age  \n",
      "0     2    Xavier      Simon   40  \n",
      "1     1     Sarah      Nayer   42  \n",
      "2     0    Andrea      Salim   26  \n",
      "3     1     Kasim      Akram   37  \n",
      "4     1     Henry  Sebastein   34  \n",
      "5     0    Amenda     Bailey   30  \n",
      "6     2     Susan       Paul   32  \n",
      "7     0      Tham      Khong   28  \n",
      "8     3      Alex      Chang   30  \n",
      "9     1       Kim       Wang   43  \n",
      "\n",
      "Average age of this org:  34.2\n",
      "Team\n",
      "Business Development    139852.0\n",
      "Client Services         233944.0\n",
      "Engineering              95570.0\n",
      "Finance                 356627.0\n",
      "Human Resources         112807.0\n",
      "Legal                   102508.0\n",
      "Marketing                97308.0\n",
      "Product                 177213.0\n",
      "Sales                        0.0\n",
      "Name: Salary, dtype: float64 <class 'pandas.core.series.Series'>\n",
      "\n",
      "                         max       mean\n",
      "Team                                   \n",
      "Business Development   7.524   7.524000\n",
      "Client Services          NaN        NaN\n",
      "Engineering           18.523  18.523000\n",
      "Finance               14.543  11.913667\n",
      "Human Resources        7.369   7.369000\n",
      "Legal                 12.637  11.381000\n",
      "Marketing              6.945   6.945000\n",
      "Product               10.012   5.894000\n",
      "Sales                  5.831   5.831000 \n",
      " <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# problem 2.6\n",
    "data = pd.DataFrame({'EmployeeName': ['Xavier Simon', 'Sarah Nayer', 'Andrea Salim', 'Kasim Akram', 'Henry Sebastein', 'Amenda Bailey', 'Susan Paul', 'Tham Khong', 'Alex Chang', 'Kim Wang'],\n",
    "                    'Department': ['Accounting', 'Engineering', 'Engineering', 'HR', 'HR', 'HR', 'Data Science', 'Data Science', 'Accounting', 'Data Science'],\n",
    "                    'HireDate': [2010, 2018, 2012, 2014, 2014, 2018, 2020, 2018, 2020, 2012],\n",
    "                    'Sex': ['M', 'F', 'F', 'M', 'M', 'F', 'F', 'M', 'M', 'F'],\n",
    "                    'Birthdate': ['04/09/1982', '14/04/1981', '06/05/1997', '08/01/1986', '10/10/1988', '12/11/1992', '10/04/1991', '16/07/1995', '08/10/1992', '11/10/1979'],\n",
    "                    'Weight': [78, 80, 66, 67, 90, 57, 115, 87, 95, 57],\n",
    "                    'Height': [176, 160, 169, 157, 185, 164, 195, 180, 174, 165],\n",
    "                    'Kids': [2, 1, 0, 1, 1, 0, 2, 0, 3, 1]\n",
    "                    })\n",
    "print(data)\n",
    "print()\n",
    "# 1 Create two columns by separating the first and the last names of the employees\n",
    "# In this case we apply lambda func to Emp name column and split them.\n",
    "\n",
    "data['FirstName'] = data['EmployeeName'].apply(lambda x : x.split()[0])\n",
    "data['LastName'] = data['EmployeeName'].apply(lambda x : x.split()[1])\n",
    "print(data)\n",
    "print()\n",
    "# 2 Compute the age of the employees and store as a separate column\n",
    "\n",
    "# We need to write a function furst to derive the age of a person\n",
    "# We need to use the date format in the dd/mm/yyyy format;\n",
    "# We use the following\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "def compute_age(birthdate):\n",
    "    birthdate = datetime.strptime(birthdate, '%d/%m/%Y').date()\n",
    "    today = date.today()\n",
    "    return today.year - birthdate.year - (today.month < birthdate.month)\n",
    "\n",
    "# now we can apply the above func to all under 'Birthdate' column in our DF\n",
    "\n",
    "data['Age'] = data['Birthdate'].apply(compute_age)\n",
    "print(data)\n",
    "print()\n",
    "# 3 Compute the average age of the employees in that organization.\n",
    "print('Average age of this org: ', data['Age'].mean())\n",
    "'''\n",
    "#=======================\n",
    "\n",
    "\n",
    "# Other useful methods to derive additional knowledge about the dataset\n",
    "'''\n",
    "\n",
    "# Aggregate method() - very useful method to collect group stats!\n",
    "\n",
    "# An aggregate is a function where the values of multiple rows are\n",
    "# grouped together to form a single summary value.\n",
    "\n",
    "# Some of the aggregate functions supported by pandas are -\n",
    "# DataFrame.aggregate(), Series.aggregate(), DataFrameGroupBy.aggregate()\n",
    "\n",
    "# We can use pandas DataFrame.aggregate() function to\n",
    "# calculate any aggregations on the selected columns of DataFrame\n",
    "# and apply multiple aggregations at the same time.  \n",
    "\n",
    "# Single aggregation - Find the total amount of salary in each team.\n",
    "\n",
    "ESD = pd.read_csv(\"EmpSmalldata.csv\")\n",
    "sal_agg = ESD.groupby('Team')['Salary'].aggregate('sum')\n",
    "print(sal_agg, type(sal_agg))\n",
    "print()\n",
    "\n",
    "# Multiple aggregation - Determine the max and mean\n",
    "\n",
    "bonus_agg = ESD.groupby('Team')['Bonus %'].aggregate(['max', 'mean'])\n",
    "print(bonus_agg, '\\n', type(bonus_agg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         max     min       mean\n",
      "Team                                           \n",
      "Business Development   7.524   7.524   7.524000\n",
      "Client Services          NaN     NaN        NaN\n",
      "Engineering           18.523  18.523  18.523000\n",
      "Finance               14.543   9.340  11.913667\n",
      "Human Resources        7.369   7.369   7.369000\n",
      "Legal                 12.637  10.125  11.381000\n",
      "Marketing              6.945   6.945   6.945000\n",
      "Product               10.012   1.256   5.894000\n",
      "Sales                  5.831   5.831   5.831000 \n",
      " <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ESD = pd.read_csv('EmpSmalldata.csv')\n",
    "\n",
    "bonus_agg = ESD.groupby('Team')['Bonus %'].aggregate(['max', 'min', 'mean'])\n",
    "print(bonus_agg, '\\n', type(bonus_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Accessing Feature1 in DF2 using my_df_keys: \n",
      " 0    0.79\n",
      "1    0.90\n",
      "2    0.08\n",
      "3    0.30\n",
      "4    0.80\n",
      "Name: Feature1, dtype: float64\n",
      "\n",
      "Accessing 3rd and 4th rows in DF2 using my_df_keys \n",
      "      Id  Feature1  Feature3  Feature2\n",
      "2  Tag6      0.08       4.7       4.7\n",
      "3  Tag7      0.30       1.2       1.2\n",
      "\n",
      "     Id  Feature1\n",
      "0  Tag1      0.79\n",
      "1  Tag2      0.90\n",
      "2  Tag6      0.08\n",
      "3  Tag7      0.30\n",
      "4  Tag8      0.80\n",
      "\n",
      "     Id  Feature1\n",
      "2  Tag6      0.08\n",
      "3  Tag7      0.30\n",
      "\n",
      "Concat column-wise: \n",
      "      Id  Feature1  Feature3    Id  Feature1  Feature2  Feature3\n",
      "0  Tag1      0.54       2.4  Tag1      0.79       7.4       7.9\n",
      "1  Tag2      0.99       5.6  Tag2      0.90       5.6      15.6\n",
      "2  Tag3      0.01       9.1  Tag6      0.08       4.7       4.7\n",
      "3  Tag4      0.30       1.2  Tag7      0.30       1.2       1.2\n",
      "4  Tag5      0.50       3.1  Tag8      0.80       3.1       3.1\n",
      "\n",
      "\n",
      "df3 is: \n",
      "      Id  Feature3  Feature4\n",
      "0  Tag1      0.99       5.1\n",
      "1  Tag2      0.00       1.6\n",
      "2  Tag3     10.00       8.0\n",
      "3  Tag4      0.70       1.0\n",
      "4  Tag5      3.50       6.0\n",
      "\n",
      "Concat column-wise: \n",
      "      Id  Feature1  Feature3    Id  Feature3  Feature4\n",
      "0  Tag1      0.54       2.4  Tag1      0.99       5.1\n",
      "1  Tag2      0.99       5.6  Tag2      0.00       1.6\n",
      "2  Tag3      0.01       9.1  Tag3     10.00       8.0\n",
      "3  Tag4      0.30       1.2  Tag4      0.70       1.0\n",
      "4  Tag5      0.50       3.1  Tag5      3.50       6.0\n",
      "(5, 6)\n",
      "Concat column-wise: \n",
      "    Feature1  Feature3  Feature3  Feature4\n",
      "0      0.54       2.4      0.99       5.1\n",
      "1      0.99       5.6      0.00       1.6\n",
      "2      0.01       9.1     10.00       8.0\n",
      "3      0.30       1.2      0.70       1.0\n",
      "4      0.50       3.1      3.50       6.0\n"
     ]
    }
   ],
   "source": [
    "# problem 2.7\n",
    "data1 = {\n",
    "        'Id': ['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5'],\n",
    "        'Feature1': [0.54,0.99,0.01,0.3,0.5],\n",
    "        'Feature3': [2.4,5.6,9.1,1.2,3.1]}\n",
    "\n",
    "# convert the above dict to a DF\n",
    "df1 = pd.DataFrame(data1, columns = ['Id', 'Feature1', 'Feature3'])\n",
    "\n",
    "# print('df1 is: \\n', df1)\n",
    "# print()\n",
    "\n",
    "data2 = {\n",
    "        'Id': ['Tag1', 'Tag2', 'Tag6', 'Tag7', 'Tag8'],\n",
    "        'Feature1': [0.79,0.90,0.08,0.3,0.8],\n",
    "        'Feature2': [7.4,5.6,4.7,1.2,3.1],\n",
    "        'Feature3': [7.9,15.6,4.7,1.2,3.1]}\n",
    "\n",
    "df2 = pd.DataFrame(data2, columns = ['Id', 'Feature1', 'Feature2','Feature3'])\n",
    "\n",
    "# print('df2 is: \\n', df2)\n",
    "# print()\n",
    "\n",
    "# Observe that number of features in both the datasets are NOT identical\n",
    "# Having created 2 DFs, we now concatenate using concat()\n",
    "\n",
    "df_row = pd.concat([df1, df2])\n",
    "\n",
    "# First observe that we are passing the DFs as a list\n",
    "\n",
    "# print(\"Concatenated dataframe is: \\n\", df_row) # what do you observe??\n",
    "\n",
    "# When you concatenate, the operation appends an extra column index, which is\n",
    "# not in sequence! \n",
    "\n",
    "\n",
    "#To fix this, we can use the parameter \"ignore_index\" in\n",
    "# the concat() method to True, which then runs the numbers in sequence.\n",
    "print()\n",
    "\n",
    "df_row_reindex = pd.concat([df1, df2], ignore_index=True) \n",
    "\n",
    "# print(\"Concatenated dataframe with correct indexing sequence is: \\n\", df_row_reindex)\n",
    "# what do you observe??\n",
    "print()\n",
    "\n",
    "#  Now we have concatenated, how do we know which data came from which\n",
    "# dataframe after concatenation?\n",
    "\n",
    "# To solve this issue, we have an option to label the DataFrames AFTER the\n",
    "# concatenation, using any specific key so that we know which data came\n",
    "# from which dataframe\n",
    "\n",
    "myframes = [df1,df2]\n",
    "#print('myframes type is: \\n',type(myframes)) \n",
    "\n",
    "my_df_keys = pd.concat(myframes, keys=['d1', 'd2'])\n",
    "# print(\"Concatenated dataframe identifying the DFs with labels \\n\", my_df_keys)\n",
    "\n",
    "print()\n",
    "\n",
    "# This helps to retrieve the individual DFs, if needed later\n",
    "# print(\"Retrieving second dataframe from concatenated dataframe: \\n\", my_df_keys.loc['d2'])\n",
    "# the above retrieve the 2nd df - Compare with df2\n",
    "\n",
    "print()\n",
    "\n",
    "print('Accessing Feature1 in DF2 using my_df_keys: \\n', my_df_keys.loc['d2']['Feature1'])\n",
    "print()\n",
    "print('Accessing 3rd and 4th rows in DF2 using my_df_keys \\n',my_df_keys.loc['d2'][2:4])\n",
    "print()\n",
    "\n",
    "# Accessing Feature1 in DF2 using my_df_keys along with their Ids\n",
    "X = my_df_keys.loc['d2']  # note that X is a DF\n",
    "#print(type(X))\n",
    "print(X.loc[:,['Id','Feature1']])\n",
    "print()\n",
    "\n",
    "# How do I access only rows 2 and 3 of DF2 using my_df_keys along with IDs?\n",
    "print(X.loc[2:3,['Id','Feature1']])\n",
    "#\n",
    "print()\n",
    "\n",
    "# ===== Concat along the columns  =====\n",
    "\n",
    "# Adding more features if you need\n",
    "# this is done by setting axis paramtere to 1;\n",
    "\n",
    "df_cols = pd.concat([df1,df2], axis=1) # axis =1 refers to col\n",
    "print(\"Concat column-wise: \\n\", df_cols)\n",
    "print()\n",
    "\n",
    "# But if you have another experiment with different features FOR\n",
    "# THE SAME SAMPLES then it will be meaningful to concatenate along the columns\n",
    "print()\n",
    "\n",
    "data3 = {\n",
    "        'Id': ['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5'],\n",
    "        'Feature3': [0.99,0,10,0.7,3.5],\n",
    "        'Feature4': [5.1,1.6,8,1,6]}\n",
    "\n",
    "# first convert the above dict to a DF as usual \n",
    "df3 = pd.DataFrame(data3, columns = ['Id', 'Feature3', 'Feature4'])\n",
    "\n",
    "print('df3 is: \\n', df3)\n",
    "print()\n",
    "\n",
    "df_cols = pd.concat([df1,df3], axis=1) # axis =1 refers to col\n",
    "print(\"Concat column-wise: \\n\", df_cols) # what do you observe?\n",
    "\n",
    "print(df_cols.shape)\n",
    "# we will remove the redundant second id col appearing now\n",
    "# But the following removes BOTH the Id columns. Run and see!\n",
    "\n",
    "df_cols.drop(df_cols.columns[[3]], axis = 1, inplace = True)\n",
    "#what does this inplace do?\n",
    "\n",
    "print(\"Concat column-wise: \\n\", df_cols)\n",
    "\n",
    "# DIY! Amend the Id column to the above df_cols DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-to-One join operation\n",
      "\n",
      "Individual dataframes are: \n",
      "   employee        group\n",
      "0      Bob   Accounting\n",
      "1     John  Engineering\n",
      "2     Lisa  Engineering\n",
      "3    Susan           HR \n",
      " \n",
      "   employee  hire_date\n",
      "0     Lisa       2004\n",
      "1      Bob       2008\n",
      "2     John       2012\n",
      "3    Susan       2014\n",
      "\n",
      "Merged dataframe df3 (1 to 1 join): \n",
      "   employee        group  hire_date\n",
      "0      Bob   Accounting       2008\n",
      "1     John  Engineering       2012\n",
      "2     Lisa  Engineering       2004\n",
      "3    Susan           HR       2014\n",
      "======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# problem 2.8.1\n",
    "print(\"One-to-One join operation\\n\")\n",
    "\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'John', 'Lisa', 'Susan'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'John', 'Susan'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "print(\"Individual dataframes are: \\n\", df1, \"\\n \\n\", df2)\n",
    "print()\n",
    "\n",
    "df3 = pd.merge(df1, df2)\n",
    "print(\"Merged dataframe df3 (1 to 1 join): \\n\",df3)\n",
    "print(\"======\\n\")\n",
    "\n",
    "# Observe that the order of entries in each column is not necessarily\n",
    "# maintained; In our example, the order of the \"employee\" column differs\n",
    "# in df1 and df2, and the merged df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many-to-One join operation\n",
      "\n",
      "Individual dataframes are: \n",
      "   employee        group  hire_date\n",
      "0      Bob   Accounting       2008\n",
      "1     John  Engineering       2012\n",
      "2     Lisa  Engineering       2004\n",
      "3    Susan           HR       2014 \n",
      " \n",
      "          group supervisor\n",
      "0   Accounting       Mike\n",
      "1  Engineering    Gillman\n",
      "2           HR      Roger \n",
      "\n",
      "Merged dataframes df3 and df4 (Many to one):\n",
      "   employee        group  hire_date supervisor\n",
      "0      Bob   Accounting       2008       Mike\n",
      "1     John  Engineering       2012    Gillman\n",
      "2     Lisa  Engineering       2004    Gillman\n",
      "3    Susan           HR       2014      Roger\n",
      "======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# problem 2.8.2\n",
    "print(\"Many-to-One join operation\\n\")\n",
    "\n",
    "# Many-to-one joins are joins in which one of the two key columns contains\n",
    "# duplicate entries. For the many-to-one case, the resulting DataFrame\n",
    "# will preserve those duplicate entries as appropriate. \n",
    "\n",
    "# Let us add a \"supervisor\" column now and merge with df3 above.\n",
    "\n",
    "grpsup = {'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Mike', 'Gillman', 'Roger']}\n",
    "\n",
    "df4 = pd.DataFrame(grpsup)\n",
    "print(\"Individual dataframes are: \\n\", df3, \"\\n \\n\", df4, \"\\n\")\n",
    "\n",
    "# duplicate column is 'group' between df3 and df4; This duplicate\n",
    "# col will be preserved when we merge and entry duplicates are\n",
    "# also preseved ('Gillman' in this case). See below. \n",
    "\n",
    "df5 = pd.merge(df3, df4)\n",
    "print(\"Merged dataframes df3 and df4 (Many to one):\\n\", df5) # What do you observe?\n",
    "print(\"======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many-to-Many join operation\n",
      "\n",
      "Individual dataframes df1 and df6 are: \n",
      "   employee        group\n",
      "0      Bob   Accounting\n",
      "1     John  Engineering\n",
      "2     Lisa  Engineering\n",
      "3    Susan           HR \n",
      " \n",
      "          group        skills\n",
      "0   Accounting          math\n",
      "1   Accounting  spreadsheets\n",
      "2  Engineering        coding\n",
      "3  Engineering         linux\n",
      "4           HR  spreadsheets\n",
      "5           HR  organization \n",
      "\n",
      " Dataframe df7 - Merged df1 and df6 is: \n",
      "   employee        group        skills\n",
      "0      Bob   Accounting          math\n",
      "1      Bob   Accounting  spreadsheets\n",
      "2     John  Engineering        coding\n",
      "3     John  Engineering         linux\n",
      "4     Lisa  Engineering        coding\n",
      "5     Lisa  Engineering         linux\n",
      "6    Susan           HR  spreadsheets\n",
      "7    Susan           HR  organization\n",
      "\n",
      "Merging df5 and df6\n",
      "   employee        group  hire_date supervisor        skills\n",
      "0      Bob   Accounting       2008       Mike          math\n",
      "1      Bob   Accounting       2008       Mike  spreadsheets\n",
      "2     John  Engineering       2012    Gillman        coding\n",
      "3     John  Engineering       2012    Gillman         linux\n",
      "4     Lisa  Engineering       2004    Gillman        coding\n",
      "5     Lisa  Engineering       2004    Gillman         linux\n",
      "6    Susan           HR       2014      Roger  spreadsheets\n",
      "7    Susan           HR       2014      Roger  organization\n"
     ]
    }
   ],
   "source": [
    "print(\"Many-to-Many join operation\\n\")\n",
    "\n",
    "# Many-to-many join operations are a bit confusing conceptually, but are \n",
    "# well defined.\n",
    "\n",
    "# Criteria is:\n",
    "# If a DF contains key columns in which both the left and right\n",
    "# array contains duplicates, then the result is a many-to-many merge.\n",
    "\n",
    "# Let us add a skill set requirement in the groups for the above data\n",
    "\n",
    "grpskills = {'group': ['Accounting', 'Accounting', 'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "             'skills': ['math', 'spreadsheets', 'coding', 'linux', 'spreadsheets', 'organization']}\n",
    "\n",
    "df6 = pd.DataFrame(grpskills)\n",
    "\n",
    "print(\"Individual dataframes df1 and df6 are: \\n\", df1, \"\\n \\n\", df6, \"\\n\")\n",
    "\n",
    "df7 = pd.merge(df1,df6)\n",
    "print(' Dataframe df7 - Merged df1 and df6 is: \\n', df7)\n",
    "print()\n",
    "\n",
    "# So, by performing a many-to-many join, we can recover the skills\n",
    "# associated with any individual person\n",
    "\n",
    "\n",
    "# Now merge df5 and df6 and see! What do you observe?\n",
    "\n",
    "print(\"Merging df5 and df6\\n\", pd.merge(df5, df6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  employee\n",
      "0      Bob\n",
      "1      Bob\n",
      "2     John\n",
      "3     John\n",
      "4     Lisa\n",
      "5     Lisa\n",
      "6    Susan\n",
      "7    Susan\n",
      "\n",
      "List of employees \n",
      " ['Bob', 'Bob', 'John', 'John', 'Lisa', 'Lisa', 'Susan', 'Susan']\n",
      "\n",
      "Dataframe df7 with a redundant (emp) col and names as indices is: \n",
      "       employee        group        skills\n",
      "Bob        Bob   Accounting          math\n",
      "Bob        Bob   Accounting  spreadsheets\n",
      "John      John  Engineering        coding\n",
      "John      John  Engineering         linux\n",
      "Lisa      Lisa  Engineering        coding\n",
      "Lisa      Lisa  Engineering         linux\n",
      "Susan    Susan           HR  spreadsheets\n",
      "Susan    Susan           HR  organization\n",
      "\n",
      "             group        skills\n",
      "Bob     Accounting          math\n",
      "Bob     Accounting  spreadsheets\n",
      "John   Engineering        coding\n",
      "John   Engineering         linux\n",
      "Lisa   Engineering        coding\n",
      "Lisa   Engineering         linux\n",
      "Susan           HR  spreadsheets\n",
      "Susan           HR  organization\n",
      "\n",
      "          group        skills\n",
      "Bob  Accounting          math\n",
      "Bob  Accounting  spreadsheets\n"
     ]
    }
   ],
   "source": [
    "# From our df7, query on Bob and retrieve all the info - Try!\n",
    "\n",
    "# As indices are not names, we can make that first\n",
    "\n",
    "namelist = df7.loc[:,['employee']]\n",
    "print(namelist) # this is still NOT a list\n",
    "print()\n",
    "namelist = namelist[\"employee\"].tolist()\n",
    "print(\"List of employees \\n\", namelist)\n",
    "print()\n",
    "# now you can reindex using namelist!\n",
    "df7.index = namelist  # same as P.P 2.1\n",
    "\n",
    "print(\"Dataframe df7 with a redundant (emp) col and names as indices is: \\n\", df7)\n",
    " \n",
    "print()\n",
    "# now we will remove the employee column from df7\n",
    "\n",
    "df7 = df7.drop(df7.columns[0],axis=1)\n",
    "print(df7)\n",
    "print()\n",
    "\n",
    "# Now query!! Uhh!! :) \n",
    "\n",
    "print(df7.loc['Bob'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
